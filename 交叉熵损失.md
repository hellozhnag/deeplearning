# 熵
信息论中熵的概念首次被香农提出，目的是寻找一种高效/无损地编码信息的方法：以编码后数据的平均长度来衡量高效性，平均长度越小越高效；同时还需满足“无损”的条件，即编码后不能有原始信息的丢失。这样，香农提出了熵的定义：**无损编码事件信息的最小平均编码长度。**

面以一个例子来说明：假设我们采用二进制编码东京的天气信息，并传输至纽约，其中东京的天气状态有4种可能，对应的概率如下图，每个可能性需要1个编码，东京的天气共需要4个编码。让我们采用3种编码方式，并对比下编码长度。不难发现，方式3编码长度最小，且是平均意义上的最小。方式3胜出的原因在于：对高可能性事件(Fine,Cloudy)用短编码，对低可能性事件(Rainy,Snow)用长编码。表中的3种方式，像是尝试的过程，那么能否直接计算出服从某一概率分布的事件的最小平均编码长度呢？还句话说，能不能直接计算熵？
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713662204384-dbbefbfa-9f37-4242-8bec-5bdd53f938fc.webp)

# 计算熵
假设一个信息事件有8种可能的状态，且各状态等可能性，即可能性都是12.5%=1/8。我们需要多少位来编码8个值呢？1位可以编码2个值(0或1)，2位可以编码2×2=4个值(00,01,10,11)，则8个值需要3位，2×2×2=8(000,001,010,011,100,101,110,111)。
> 在信息论中，如果所有符号出现的概率完全相等，通常我们不需要使用变长编码，因为在这种情况下，每个符号分配相等长度的编码可以获得最优的平均编码长度，并且编码和解码过程简单。

对于具有N种等可能性状态的信息，每种状态的可能性P = 1/N，编码该信息所需的最小编码长度为：
$\log_{2}{N}=\log_{2}{\frac{1}{N}}=-\log_{2}{P}$

那么计算平均最小长度，也就是熵的公式为：
$Entropy=-\sum_{i=0}^nP(i)*\log_{2}{P(i)}$
回头看看上述编码东京天气的例子，熵 = 1 * 50% + 2 * 25% + 3 * 12.5% + 3 * 12.5% = 1.75
![image.png](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713663721933-fd5caa34-8c00-4c88-afe1-622b42314050.png)

**熵的直观解释：**
如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。
并且当一个罕见的信息到达时，比一个常见的信息有着更多的信息量，因为它排除了别的很多的可能性，告诉了我们一个确切的信息。在天气的例子中，Rainy发生的概率为12.5%，当接收到该信息时，我们减少了87.5%的不确定性(Fine,Cloudy,Snow)；如果接收到Fine(50%)的消息，我们只减少了50%的不确定性。

# 交叉熵
分类模型中会使用交叉熵作损失函数
上文中已知一个离散变量 i 的概率分布P(i)的熵的公式表示
同理，对于连续变量 x 的概率分布P(x)，熵的公式可以表示为：
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664025512-1b7d5dcf-92a3-474a-adce-690b1e37bb0d.webp)
在熵的公式中，对于离散变量和连续变量，我们都是计算了 负的可能性的对数 的期望，代表了该事件理论上的平均最小编码长度，所以熵的公式也可表示如下，公式中的x~P代表我们使用概率分布P来计算期望，熵又可以简写为H：
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664084349-e23a7185-2c52-4f4a-8d6d-1a6b10c81f95.png)
熵是服从某一特定概率分布事件的理论最小平均编码长度”，只要我们知道了任何事件的概率分布，我们就可以计算它的熵；那如果我们不知道事件的概率分布，又想计算熵，该怎么做呢？那我们来对熵做一个估计吧，熵的估计的过程自然而然的引出了交叉熵。
**熵的估计：**
假如我们现在需要预报东京天气，在真实天气发生之前，我们不可能知道天气的概率分布；但为了下文的讨论，我们需要假设：对东京天气做一段时间的观测后，可以得到真实的概率分布P。
在观测之前，我们只有预估的概率分布Q，使用估计得到的概率分布，可以计算估计的熵：
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664308302-29b38524-38da-4be4-b9d9-066815095d61.webp)
如果Q是真实的概率分布，根据上述公式，我们已经得到了编码东京天气信息的最小平均长度；然而估计的概率分布为我们的公式引入了两部分的不确定性：

- 计算期望的概率分布是Q，与真实的概率分布P不同。
- 计算最小编码长度的概率是 -logQ，与真实的最小编码长度 -logP 不同。

因为估计的概率分布Q影响了上述两个部分(期望和编码长度)，所以得到的结果很可能错得离谱，也因此该结果和真实熵的对比无意义。和香农的目标一样，我们希望编码长度尽可能的短，所以我们需要对比我们的编码长度和理论上的最小编码长度(熵)。假设经过观测后，我们得到了真实概率分布P，在天气预报时，就可以使用P计算平均编码长度，实际编码长度基于Q计算，这个计算结果就是P和Q的交叉熵。这样，实际编码长度和理论最小编码长度就有了对比的意义。
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664361497-9ad1f486-05f7-42ee-851d-82a0e9a5b25f.webp)
交叉熵使用H(P,Q)表示，意味着使用P计算期望，使用Q计算编码长度；所以H(P,Q)并不一定等于H(Q,P)，除了在P=Q的情况下，H(P,Q) = H(Q,P) = H(P)。

有一点很微妙但很重要：对于期望，我们使用真实概率分布P来计算；对于编码长度，我们使用假设的概率分布Q来计算，因为它是预估用于编码信息的。因为熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵。换句话说，如果我们的估计是完美的，即Q=P，那么有H(P,Q) = H(P)，否则，H(P,Q) > H(P)。

# 交叉熵作为损失函数
假设一个动物照片的数据集中有5种动物，且每张照片中只有一只动物，每张照片的标签都是one-hot编码。
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664456973-115e1d1c-a950-4783-b84b-da04c5bddf98.png)
假设有两个机器学习模型对第一张照片分别作出了预测：Q1和Q2,而第一张照片的真实标签为[1,0,0,0,0]。
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664491103-94c8b2cf-738c-4764-94d2-9fc914d36b3b.webp)
两个模型预测效果如何呢，可以分别计算下交叉熵：
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664525931-10ef225f-7c08-4608-87ca-02bd0d730d09.webp)
交叉熵对比了模型的预测结果和数据的真实标签，随着预测越来越准确，交叉熵的值越来越小，如果预测完全正确，交叉熵的值就为0。因此，训练分类模型时，可以使用交叉熵作为损失函数。

**二分类交叉熵：**
在二分类模型中，标签只有是和否两种；这时，可以使用二分类交叉熵作为损失函数。假设数据集中只有猫和狗的照片，则交叉熵公式中只包含两种可能性：
> P是真实概率分布，Q是训练预测的

![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664601974-d8fea2a2-dbd2-4944-8776-16a421f1a3b0.webp)
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664653700-48e77a94-9747-4aae-9035-a1bb5ceb0d27.png)
![](./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.images/1713664740464-50c92db1-8343-426c-b9aa-43f7649d575d.webp)

